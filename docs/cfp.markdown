---
layout: default
title: Call for Papers
permalink: /cfp/
---

# Call for Papers

Simulation techniques are not foreign to information retrieval.  Simulation has been employed, for example, for constructing test collections and for model performance prediction and analysis in a broad array of information access scenarios.  The need for simulation has become ever more apparent recently with the emergence of areas where other types of evaluation are infeasible.  One such area is conversational information access, where human evaluation is both time and resource intensive at scale.  Another example is provided by settings that do not allow sharing of data, e.g., because of privacy constraints, and therefore necessitate the creation of synthetic test collections.

Despite the apparent need, a standardized methodology for performance evaluation via simulation has not yet been developed.  The goal of the Sim4IR workshop is to create a forum for researchers and practitioners to promote methodology development and more widespread use of simulation for evaluation by:

1. identifying problem settings and application scenarios;
2. sharing tools, techniques, and experiences;
3. characterizing potentials and limitations; and
4. developing a research agenda.

We invite submissions of regular papers, position papers, demonstrators as well as encore talks (featuring already published work).

## Topics of Interest

Topics of interest include, *but are not limited to:*

* Problem settings and application scenarios that lend themselves to evaluation via simulation, for example
  * Simulation of users and user interactions
  * Synthetic test collections
* Characterizing the capabilities and limitations of simulation approaches for various IR problems
  * Simulation methods, tools, techniques, and toolkits
  * Evaluation of simulation